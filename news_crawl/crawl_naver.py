import os
import requests
from bs4 import BeautifulSoup
import csv
import schedule
import time

# ë‰´ìŠ¤ íŒŒì¼ ì €ì¥ í´ë” ì„¤ì •
NEWS_DIR = "news_crawl/news_data"
if not os.path.exists(NEWS_DIR):
    os.makedirs(NEWS_DIR)

# í¬ë¡¤ë§ í•¨ìˆ˜ ì •ì˜
def crawl_news():
    print("\nğŸš€ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")

    # ë„¤ì´ë²„ IT ë‰´ìŠ¤ URL
    url = "https://news.naver.com/main/main.naver?mode=LSD&mid=shm&sid1=105"

    # HTTP ìš”ì²­
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")

    # ë‰´ìŠ¤ ì œëª© ê°€ì ¸ì˜¤ê¸°
    titles = soup.select(".sa_text_strong")

    # ë‚ ì§œì™€ ì‹œê°„ë³„ íŒŒì¼ëª… ìƒì„± (ì‹œê°„ ë‹¨ìœ„ í¬í•¨)
    today = time.strftime('%Y-%m-%d_%H')
    filename = os.path.join(NEWS_DIR, f"news_{today}.csv")

    # CSV íŒŒì¼ë¡œ ì €ì¥
    with open(filename, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["ë²ˆí˜¸", "ì œëª©", "ë§í¬"])  # í—¤ë” ì¶”ê°€

        for idx, title in enumerate(titles, 1):
            news_title = title.text.strip()
            news_link = title.find_parent("a")["href"]
            writer.writerow([idx, news_title, news_link])

    print(f"âœ… ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ! ({len(titles)}ê°œ ë‰´ìŠ¤ ì €ì¥ë¨)")
    with open("news_crawl/log.txt", "a", encoding="utf-8") as log_file:
        log_file.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] ë‰´ìŠ¤ í¬ë¡¤ë§ ì™„ë£Œ! íŒŒì¼: {filename}\n")

# ğŸ’¡ 10ë¶„ë§ˆë‹¤ ì‹¤í–‰ë˜ë„ë¡ ì„¤ì •
schedule.every(1).minutes.do(crawl_news)

# ë¬´í•œ ë£¨í”„ ì‹¤í–‰ (ê³„ì† ë™ì‘)
print("â³ ìë™ í¬ë¡¤ë§ ì‹œì‘! (Ctrl + Cë¡œ ì¢…ë£Œ ê°€ëŠ¥)")
while True:
    schedule.run_pending()
    time.sleep(1)